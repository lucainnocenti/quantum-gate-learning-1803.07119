{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.57121973,   4.72067853,   8.04580421,   2.13757128],\n",
       "       [ 18.6612217 ,  16.40946115,  21.30658595,   9.53122445],\n",
       "       [  5.28795884,   5.08425043,   6.09091051,   2.89564534],\n",
       "       [ 15.03470264,  15.63105922,  18.65004717,   9.20063013]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  8.56549989,   4.71673067,   8.0400172 ,   2.1355651 ],\n",
       "       [ 18.64621668,  16.39642035,  21.28964497,   9.52387624],\n",
       "       [  5.28352969,   5.08035571,   6.08585347,   2.89338313],\n",
       "       [ 15.02161972,  15.61897565,  18.63482564,   9.19362008]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.linalg as la\n",
    "import theano\n",
    "import theano.tensor.slinalg\n",
    "import theano.tensor as T\n",
    "\n",
    "n = 4\n",
    "\n",
    "A = np.zeros(shape=(n, n), dtype=np.float)\n",
    "A += scipy.sparse.rand(n, n, density=.8)\n",
    "A = np.asarray(A)\n",
    "\n",
    "\n",
    "# ----------- numerical differentiation\n",
    "x = 2.\n",
    "h = 0.001\n",
    "numerical_diff = (la.expm((x + h) * A) - la.expm(x * A)) / h\n",
    "\n",
    "# display gradient computed with numerical differentiation\n",
    "display(numerical_diff)\n",
    "\n",
    "\n",
    "\n",
    "# ----------- automatic differentiation with theano\n",
    "x = T.dscalar('x')\n",
    "expA = T.slinalg.expm(x * A)\n",
    "# the flattening is for more easily scan through the elements of the matrix\n",
    "# (theano.grad only accepts scalar cost)\n",
    "expA_flat = T.flatten(expA)\n",
    "\n",
    "def compute_element_grad(idx, flattened_matrix):\n",
    "    return T.grad(flattened_matrix[idx], wrt=x)\n",
    "# `theano.scan` basically loops over the elements of the matrix, and returns the gradient of each \n",
    "g_x_flat, _ = theano.scan(\n",
    "    fn=compute_element_grad,\n",
    "    sequences=T.arange(expA_flat.shape[0]),\n",
    "    non_sequences=[expA_flat]\n",
    ")\n",
    "# deflatten result\n",
    "g_x = T.reshape(g_x_flat, newshape=expA.shape)\n",
    "\n",
    "# here is where the computational graph is actually compiled\n",
    "gradient = theano.function(inputs=[x], outputs=g_x)\n",
    "\n",
    "\n",
    "# compute and display gradient computed with AD\n",
    "display(gradient(2.))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano]",
   "language": "python",
   "name": "conda-env-theano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
